{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetReader(Dataset):\n",
    "    def __init__(self, df, dataset_name):\n",
    "        self.df = df\n",
    "        self.name = dataset_name\n",
    "        print(f\"{self.name} : {self.df.shape[0]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sd = self.df.iloc[idx]\n",
    "        user = sd['user_id']\n",
    "        item = sd['item_id']\n",
    "        rating = sd['rating']\n",
    "        return torch.tensor(user-1).long(), torch.tensor(item-1).long(), torch.tensor(rating).float()\n",
    "\n",
    "def get_datasets(bucket='datasets', dataset='ml-25m', split=['test', 'train', 'val']):\n",
    "    from pyarrow import fs, parquet\n",
    "    valid_splits = ['test', 'train', 'val']\n",
    "    data_map = {}\n",
    "    minio = fs.S3FileSystem(\n",
    "        endpoint_override='http://minio-service.kubeflow:9000',\n",
    "         access_key='minio',\n",
    "         secret_key='minio123',\n",
    "         scheme='http')\n",
    "\n",
    "    if type(split) is not list:\n",
    "        split = [split]\n",
    "\n",
    "    for dataset_name in valid_splits:\n",
    "        paraquet_data = minio.open_input_file(f'{bucket}/{dataset}/{dataset_name}.parquet.gzip')\n",
    "        df = parquet.read_table(paraquet_data).to_pandas()\n",
    "        data_map['n_users'] = max(data_map['n_users'], df.user_id.max())\n",
    "        data_map['n_items'] = max(data_map['n_items'], df.item_id.max())\n",
    "        if dataset_name in split:\n",
    "            data_map[dataset_name] = datasetReader(df, dataset_name=dataset_name)\n",
    "    \n",
    "    assert list(data_map.keys()) == split, f\"Mismatched or invalid splits. Received {split} but can only process {valid_splits}\"\n",
    "    return data_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "### Testing code.\n",
    "def negative_sampling(ratings, num_ng_test=10):\n",
    "    print('x')\n",
    "    item_pool = set(ratings['item_id'].unique())\n",
    "    print('y')\n",
    "    interact_status = (\n",
    "\t\t\tratings.groupby('user_id')['item_id']\n",
    "\t\t\t.apply(set)\n",
    "\t\t\t.reset_index()\n",
    "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
    "    print('z')\n",
    "    interact_status['negative_samples'] = interact_status['interacted_items'].apply(lambda x: np.random.choice(list(item_pool - x), num_ng_test))\n",
    "    interact_status['rating'] = 0.0\n",
    "    interact_status['timestamp'] = 1051631039\n",
    "    interact_status = interact_status.drop(columns=['interacted_items']).explode('negative_samples').rename(columns={'negative_samples':'item_id'})\n",
    "    print('a')\n",
    "    #ret = ratings.append(interact_status, ignore_index=True)\n",
    "    ret = pd.concat([ratings, interact_status], ignore_index=True)\n",
    "    return ret\n",
    "\n",
    "def split_dataset(path_to_ml_25m = \"/Users/shanoop/Downloads/ml-25m\",  random_state: int = 42):\n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\t\n",
    "    num_ng_test = 10\n",
    "    num_ng = 10\n",
    "\n",
    "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    ratings_df = pd.read_csv(os.path.join(path_to_ml_25m, 'ratings.csv'),names=names, index_col=False, skiprows=1)\n",
    "    ratings_df = ratings_df.iloc[:6400]\n",
    "    ratings_df = negative_sampling(ratings_df)\n",
    "\t\n",
    "\n",
    "    n_users = ratings_df.user_id.max()\n",
    "    n_items = ratings_df.item_id.max()\n",
    "\n",
    "    # train is now 75% of the entire data set\n",
    "    train, test = train_test_split(\n",
    "        ratings_df,                                    \n",
    "        test_size=1 - train_ratio,\n",
    "        random_state=random_state)\n",
    "\t\n",
    "    # preprocess\n",
    "    #train_ratings, test_ratings = leave_one_out(preprocess_ratings)\n",
    "\n",
    "\t\n",
    "\n",
    "    # test is now 10% of the initial data set\n",
    "    # validation is now 15% of the initial data set\n",
    "    val, test = train_test_split(   \n",
    "        test,\n",
    "        test_size=test_ratio / (test_ratio + validation_ratio),\n",
    "        random_state=random_state)\n",
    "\t\n",
    "    \n",
    "    return train, test, val, (n_users, n_items)\n",
    "\n",
    "def get_datasets_local(bucket='datasets', dataset='ml-25m', split=['test', 'train', 'val']):\n",
    "    train, test, val, (n_users, n_items) = split_dataset()\n",
    "\n",
    "    data_map = {'n_users': n_users, 'n_items': n_items, 'train': train, 'test': test, 'val': val}\n",
    "\n",
    "    for dataset_name in ['train', 'test', 'val']:\n",
    "        data_map[dataset_name] = datasetReader(data_map[dataset_name], dataset_name=dataset_name)\n",
    "    #    data_map['n_users'] = max(data_map['n_users'], df.user_id.max())\n",
    "    #    data_map['n_items'] = max(data_map['n_items'], df.item_id.max())\n",
    "    \n",
    "    assert data_map['n_users'] == n_users\n",
    "    assert data_map['n_items'] == n_items\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset(bucket:str = 'datasets', dataset:str = 'ml-25m'):\n",
    "    from pyarrow import fs, parquet\n",
    "    print(\"Running QA\")\n",
    "    minio = fs.S3FileSystem(\n",
    "        endpoint_override='http://minio-service.kubeflow:9000',\n",
    "         access_key='minio',\n",
    "         secret_key='minio123',\n",
    "         scheme='http')\n",
    "    train_parquet = minio.open_input_file(f'{bucket}/{dataset}/train.parquet.gzip')\n",
    "    df = parquet.read_table(train_parquet).to_pandas()\n",
    "    assert set(['user_id', 'item_id', 'rating']).issubset(df.columns), f'Unable to find a required column. Found {df.columns}'\n",
    "    print('QA passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "y\n",
      "z\n",
      "a\n",
      "train : 5145\n",
      "test : 686\n",
      "val : 1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/13 01:48:51 INFO mlflow.types.utils: MLflow 2.9.0 introduces model signature with new data types for lists and dictionaries. For input such as Dict[str, Union[scalars, List, Dict]], we infer dictionary values types as `List -> Array` and `Dict -> Object`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Evaluating\n",
      "Test loss: 1.1749027967453003\n",
      "Train loss: 1.4642317295074463\n",
      "1\n",
      "Evaluating\n",
      "Test loss: 1.1438854932785034\n",
      "Train loss: 1.1712555885314941\n",
      "2\n",
      "Evaluating\n",
      "Test loss: 1.121619462966919\n",
      "Train loss: 1.141432523727417\n",
      "3\n",
      "Evaluating\n",
      "Test loss: 1.102582573890686\n",
      "Train loss: 1.1218864917755127\n",
      "4\n",
      "Evaluating\n",
      "Test loss: 1.090576410293579\n",
      "Train loss: 1.113316535949707\n",
      "5\n",
      "Evaluating\n",
      "Test loss: 1.0769343376159668\n",
      "Train loss: 1.0954406261444092\n",
      "6\n",
      "Evaluating\n",
      "Test loss: 1.0638604164123535\n",
      "Train loss: 1.0800702571868896\n",
      "7\n",
      "Evaluating\n",
      "Test loss: 1.0568610429763794\n",
      "Train loss: 1.0677101612091064\n",
      "8\n",
      "Evaluating\n",
      "Test loss: 1.0546883344650269\n",
      "Train loss: 1.057148814201355\n",
      "9\n",
      "Evaluating\n",
      "Test loss: 1.04603910446167\n",
      "Train loss: 1.046576976776123\n",
      "10\n",
      "Evaluating\n",
      "Test loss: 1.0394843816757202\n",
      "Train loss: 1.037510633468628\n",
      "11\n",
      "Evaluating\n",
      "Test loss: 1.041893482208252\n",
      "Train loss: 1.042807936668396\n",
      "12\n",
      "Evaluating\n",
      "Test loss: 1.0418674945831299\n",
      "Train loss: 1.0450609922409058\n",
      "13\n",
      "Evaluating\n",
      "Test loss: 1.039739727973938\n",
      "Train loss: 1.0393441915512085\n",
      "14\n",
      "Evaluating\n",
      "Test loss: 1.0356628894805908\n",
      "Train loss: 1.0365116596221924\n",
      "15\n",
      "Evaluating\n",
      "Test loss: 1.0370134115219116\n",
      "Train loss: 1.0421054363250732\n",
      "16\n",
      "Evaluating\n",
      "Test loss: 1.0352015495300293\n",
      "Train loss: 1.0407733917236328\n",
      "17\n",
      "Evaluating\n",
      "Test loss: 1.039232850074768\n",
      "Train loss: 1.035949945449829\n",
      "18\n",
      "Evaluating\n",
      "Test loss: 1.0326886177062988\n",
      "Train loss: 1.035658597946167\n",
      "19\n",
      "Evaluating\n",
      "Test loss: 1.0376883745193481\n",
      "Train loss: 1.0393508672714233\n",
      "20\n",
      "Evaluating\n",
      "Test loss: 1.037394642829895\n",
      "Train loss: 1.0390127897262573\n",
      "21\n",
      "Evaluating\n",
      "Test loss: 1.0354524850845337\n",
      "Train loss: 1.0415167808532715\n",
      "22\n",
      "Evaluating\n",
      "Test loss: 1.0319316387176514\n",
      "Train loss: 1.0333342552185059\n",
      "23\n",
      "Evaluating\n",
      "Test loss: 1.0403177738189697\n",
      "Train loss: 1.0419408082962036\n",
      "24\n",
      "Evaluating\n",
      "Test loss: 1.031906247138977\n",
      "Train loss: 1.0356847047805786\n",
      "25\n",
      "Evaluating\n",
      "Test loss: 1.0405378341674805\n",
      "Train loss: 1.0436102151870728\n",
      "26\n",
      "Evaluating\n",
      "Test loss: 1.0344328880310059\n",
      "Train loss: 1.0372706651687622\n",
      "27\n",
      "Evaluating\n",
      "Test loss: 1.0374913215637207\n",
      "Train loss: 1.038387417793274\n",
      "28\n",
      "Evaluating\n",
      "Test loss: 1.0334210395812988\n",
      "Train loss: 1.0312093496322632\n",
      "29\n",
      "Evaluating\n",
      "Test loss: 1.0366402864456177\n",
      "Train loss: 1.036955714225769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanoop/miniforge3/envs/mlops/lib/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/Users/shanoop/miniforge3/envs/mlops/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "def train_model(mlflow_experiment_name='recommender', mlflow_run_id=None, mlflow_tags={},\n",
    "                hot_reload_model_run_id=None,\n",
    "                model_embedding_factors=20, model_learning_rate=1e-3,model_hidden_dims=256, model_dropout_rate=0.2,\n",
    "                optimizer_step_size=10, optimizer_gamma=0.1,\n",
    "                training_epochs=30,\n",
    "                train_batch_size=64, test_batch_size=64, shuffle_training_data=True, shuffle_testing_data=True):\n",
    "    input_params = {}\n",
    "    for k, v in locals().items():\n",
    "        if k == 'input_params':\n",
    "            continue\n",
    "        input_params[k] = v\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "    from torch.utils.data import DataLoader\n",
    "    import mlflow\n",
    "    from torchinfo import summary\n",
    "    from mlflow.models import infer_signature\n",
    "\n",
    "    \n",
    "    class MatrixFactorization(torch.nn.Module):\n",
    "        def __init__(self, n_users, n_items, n_factors, hidden_dim, dropout_rate):\n",
    "            super().__init__()\n",
    "            self.n_items = n_items\n",
    "            self.user_factors = torch.nn.Embedding(n_users+1, \n",
    "                                               n_factors,\n",
    "                                               sparse=False)\n",
    "            self.item_factors = torch.nn.Embedding(n_items+1, \n",
    "                                               n_factors,\n",
    "                                               sparse=False)\n",
    "        \n",
    "            self.linear = torch.nn.Linear(in_features=n_factors, out_features=hidden_dim)\n",
    "            self.linear2 = torch.nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "            self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        def forward(self, user, item):\n",
    "            user_embedding = self.user_factors(user)\n",
    "            item_embedding = self.item_factors(item)\n",
    "            embeddding_vector = torch.mul(user_embedding, item_embedding)\n",
    "            x = self.relu(self.linear(embeddding_vector))\n",
    "            x = self.dropout(x)\n",
    "            rating = self.linear2(x)\n",
    "            return rating\n",
    "    \n",
    "    dataset_map = get_datasets_local(split=['train', 'test'])\n",
    "    \n",
    "    if hot_reload_model_run_id is not None:\n",
    "        model_uri = f\"runs:/{hot_reload_model_run_id}/model\"\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "    else:\n",
    "        model = MatrixFactorization(dataset_map['n_users'], dataset_map['n_items'], n_factors=model_embedding_factors, hidden_dim=model_hidden_dims, dropout_rate=model_dropout_rate)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=model_learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=optimizer_step_size, gamma=optimizer_gamma)\n",
    "    loss_func = torch.nn.L1Loss()\n",
    "    train_dataloader = DataLoader(dataset_map['train'], batch_size=train_batch_size, shuffle=shuffle_training_data)\n",
    "    test_dataloader = DataLoader(dataset_map['test'], batch_size=test_batch_size, shuffle=shuffle_testing_data)\n",
    "\n",
    "    # Set our tracking server uri for logging\n",
    "    mlflow.set_tracking_uri(uri=\"http://192.168.1.104:8080\")\n",
    "\n",
    "    # Create a new MLflow Experiment\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "    with mlflow.start_run(run_id=mlflow_run_id):\n",
    "        for k,v in input_params.items():\n",
    "            if 'mlflow_' not in k:\n",
    "                mlflow.log_param(k, v)\n",
    "        mlflow.log_param(\"loss_function\", loss_func.__class__.__name__)\n",
    "        #mlflow.log_param(\"metric_function\", metric_fn.__class__.__name__,\n",
    "        mlflow.log_param(\"optimizer\", \"SGD\")\n",
    "        mlflow.log_params({'n_user': dataset_map['n_users'], 'n_items': dataset_map['n_items']})\n",
    "    \n",
    "        for k,v in mlflow_tags.items():\n",
    "            mlflow.set_tag(k, v)\n",
    "\n",
    "        with open(\"model_summary.txt\", \"w\") as f:\n",
    "            f.write(str(summary(model)))\n",
    "        mlflow.log_artifact(\"model_summary.txt\")\n",
    "\n",
    "        model_signature = None\n",
    "\n",
    "        for train_iter in range(training_epochs):\n",
    "            print(train_iter)\n",
    "            model.train()\n",
    "            t_loss = 0\n",
    "            t_count = 0\n",
    "            for row, col, rating in train_dataloader:\n",
    "                # Predict and calculate loss\n",
    "                #try:\n",
    "                prediction = model(row, col)\n",
    "                if model_signature is None:\n",
    "                    model_signature = infer_signature({'user': row.cpu().detach().numpy(), 'movie': col.cpu().detach().numpy()}, prediction.cpu().detach().numpy())\n",
    "\n",
    "                #except Exception as e:\n",
    "                #print(f\"R:{row}, C:{col}\")\n",
    "                loss = loss_func(prediction, rating.unsqueeze(1))\n",
    "                t_loss += loss\n",
    "                t_count += 1\n",
    "\n",
    "                # Backpropagate\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            mlflow.log_metric(\"avg_training_loss\", f\"{(t_loss/t_count):3f}\", step=train_iter)\n",
    "            scheduler.step()\n",
    "            model.eval()\n",
    "            te_loss = 0\n",
    "            te_count = 0\n",
    "            print('Evaluating')\n",
    "            with torch.no_grad():\n",
    "                #HR, NDCG = metrics(model, test_dataloader, 5)\n",
    "                for row, col,rating in test_dataloader:\n",
    "                    prediction = model(row, col)\n",
    "                    loss = loss_func(prediction, rating.unsqueeze(1))\n",
    "                    te_loss += loss\n",
    "                    te_count += 1\n",
    "            mlflow.log_metric(\"avg_testing_loss\", f\"{(te_loss/te_count):3f}\", step=train_iter)\n",
    "            #print(f\"HR: {HR} NDCG:{NDCG}\")\n",
    "            print(f\"Test loss: {te_loss/te_count}\")\n",
    "            print(f\"Train loss: {t_loss/t_count}\")\n",
    "\n",
    "        mlflow.pytorch.log_model(model, \"model\", signature=model_signature)\n",
    "\n",
    "train_model(train_batch_size=64, test_batch_size=64, training_epochs=30, model_learning_rate=1e-2, mlflow_tags={'negative_sampling': 'True', 'testing_sample': 'tracking'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6980ffe9ee4dbb847de0967bd8dfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/13 01:50:04 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "y\n",
      "z\n",
      "a\n",
      "train : 5145\n",
      "test : 686\n",
      "val : 1029\n",
      "precision_50: 0.7442\n",
      "recall_50: 0.8649\n",
      "rms: 0.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shanoop/miniforge3/envs/mlops/lib/python3.10/site-packages/sklearn/metrics/_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def validate_model(model_run_id, top_k=50, threshold=3, val_batch_size=32):\n",
    "    # https://pureai.substack.com/p/recommender-systems-with-pytorch\n",
    "    from collections import defaultdict\n",
    "    import torch\n",
    "    import mlflow.pytorch\n",
    "    import mlflow\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    mlflow.set_tracking_uri(uri=\"http://192.168.1.104:8080\")\n",
    "\n",
    "    model_uri = f\"runs:/{model_run_id}/model\"\n",
    "    recommendation_model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "    def calculate_precision_recall(user_ratings, k, threshold):\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum(true_r >= threshold for _, true_r in user_ratings)\n",
    "        n_rec_k = sum(est >= threshold for est, _ in user_ratings[:k])\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) and (est >= threshold) for est, true_r in user_ratings[:k])\n",
    "\n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "        return precision, recall\n",
    "\n",
    "    user_ratings_comparison = defaultdict(list)\n",
    "\n",
    "    dataset_map = get_datasets_local(split=['val'])\n",
    "    val_dataloader = DataLoader(dataset_map['val'], batch_size=val_batch_size, shuffle=True)\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    recommendation_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, movies, ratings in val_dataloader:\n",
    "            output = recommendation_model(users, movies)\n",
    "\n",
    "            y_pred.append(output.sum().item() / len(users))\n",
    "            y_true.append(ratings.sum().item() / len(users))\n",
    "\n",
    "            for user, pred, true in zip(users, output, ratings):\n",
    "                user_ratings_comparison[user.item()].append((pred[0].item(), true.item()))\n",
    "\n",
    "    user_precisions = dict()\n",
    "    user_based_recalls = dict()\n",
    "\n",
    "    k = top_k\n",
    "\n",
    "    for user_id, user_ratings in user_ratings_comparison.items():\n",
    "        precision, recall = calculate_precision_recall(user_ratings, k, threshold)\n",
    "        user_precisions[user_id] = precision\n",
    "        user_based_recalls[user_id] = recall\n",
    "\n",
    "\n",
    "    average_precision = sum(prec for prec in user_precisions.values()) / len(user_precisions)\n",
    "    average_recall = sum(rec for rec in user_based_recalls.values()) / len(user_based_recalls)\n",
    "    rms = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    print(f\"precision_{k}: {average_precision:.4f}\")\n",
    "    print(f\"recall_{k}: {average_recall:.4f}\")\n",
    "    print(f\"rms: {rms:.4f}\")\n",
    "    mlflow.log_metric(f\"precision_{k}\", average_precision, run_id=model_run_id)\n",
    "    mlflow.log_metric(f\"recall_{k}\", average_recall, run_id=model_run_id)\n",
    "    mlflow.log_metric(\"rms\", rms, run_id=model_run_id)\n",
    "\n",
    "validate_model('0b2bb12d3db94a938005dfe1651a1d9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model quality gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_quality_gate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'recommender_production' already exists. Creating a new version of this model...\n",
      "2024/05/13 00:49:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: recommender_production, version 4\n",
      "Created version '4' of model 'recommender_production'.\n"
     ]
    }
   ],
   "source": [
    "def promote_model_to_staging(new_model_run_id, registered_model_name='recommender_production', rms_threshold=0.0, precision_threshold=-0.3, recall_threshold=-0.2):\n",
    "    import mlflow.pytorch\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    from mlflow.exceptions import RestException\n",
    "\n",
    "    mlflow.set_tracking_uri(uri=\"http://192.168.1.104:8080\")\n",
    "    client = MlflowClient()\n",
    "\n",
    "    current_staging = None\n",
    "    try:\n",
    "        current_staging = client.get_model_version_by_alias(registered_model_name, \"staging\")\n",
    "    except RestException:\n",
    "        print(\"No staging model found. Auto upgrade current run to staging.\")\n",
    "    \n",
    "    if current_staging.run_id == new_model_run_id:\n",
    "        print(\"Input run is already the current staging.\")\n",
    "        return\n",
    "    \n",
    "    if current_staging is not None:\n",
    "        current_staging_model_data = client.get_run(current_staging.run_id).data.to_dictionary()\n",
    "        staging_model_metrics = current_staging_model_data['metrics']\n",
    "\n",
    "        new_model_data = client.get_run(new_model_run_id).data.to_dictionary()\n",
    "        new_model_metrics = new_model_data['metrics']\n",
    "\n",
    "        if (new_model_metrics['rms'] - staging_model_metrics['rms']) > rms_threshold:\n",
    "            return\n",
    "\n",
    "        if (new_model_metrics['precision_50'] - staging_model_metrics['precision_50']) < precision_threshold:\n",
    "            return\n",
    "        \n",
    "        if (new_model_metrics['recall_50'] - staging_model_metrics['recall_50']) < recall_threshold:\n",
    "            return\n",
    "\n",
    "    result = mlflow.register_model(f\"runs:/{new_model_run_id}/model\", \"recommender_production\")\n",
    "    client.set_registered_model_alias(\"recommender_production\", \"staging\", result.version)\n",
    "\n",
    "\n",
    "\n",
    "promote_model('9cc6321b8dcf4e43a1fd6b339107d10e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "client = kfp.Client() # change arguments accordingly\n",
    "@dsl.pipeline(\n",
    "  name='Model training pipeline',\n",
    "  description='A pipeline to train models on the movielens dataset for recommenders'\n",
    ")\n",
    "def training_pipeline(\n",
    "    minio_bucket:str='datasets',\n",
    "    trainig_batch_size: int = 1,\n",
    "    training_learning_rate:float = 0.001,\n",
    "    training_factors: int = 20,\n",
    "    optimizer_step_size: float= 25.0,\n",
    "    optimizer_gamma: float = 0.1):\n",
    "    check_dataset\n",
    "    train_model\n",
    "    validate_model\n",
    "    model_quality_gate\n",
    "    promote_model\n",
    "\n",
    "    download_dataset = download_op()\n",
    "    unzip_folder = unzip_op(download_dataset.output)\n",
    "    ratings_parquet_op = csv_to_parquet_op(unzip_folder.outputs['ratings_output'])\n",
    "    movies_parquet_op = csv_to_parquet_op(unzip_folder.outputs['movies_output'])\n",
    "    split_op = split_dataset_op(ratings_parquet_op.output,random_state=random_init)\n",
    "    u1 = upload_to_minio_op(movies_parquet_op.output, upload_file_name='movies.parquet.gzip', bucket=minio_bucket)\n",
    "    u2 = upload_to_minio_op(split_op.output, bucket=minio_bucket)\n",
    "    qa_component_op(bucket=minio_bucket).after(u2)\n",
    "\n",
    "# Create a pipeline run, using the client you initialized in a prior step.\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=dataprep_pipeline,\n",
    "    package_path='dataPrep_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
